{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNyQ0fIvveznpHHwYxNuxkZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EmicoBinsfinder/EPOCodeFestProject/blob/main/DrillDownV1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "AwZLm-jWYBaO"
      },
      "outputs": [],
      "source": [
        "#@title Configure OpenAI API key\n",
        "\n",
        "# access your OpenAI API key\n",
        "\n",
        "# installing llmx first isn't necessary but avoids a confusing error when installing openai\n",
        "!pip install -q llmx\n",
        "!pip install -q openai\n",
        "from openai import OpenAI\n",
        "import google.generativeai as genai\n",
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "openai_api_secret_name = 'Test'\n",
        "## @param {type: \"string\"}\n",
        "\n",
        "try:\n",
        "  OPENAI_API_KEY=userdata.get(openai_api_secret_name)\n",
        "  client = OpenAI(\n",
        "    api_key=OPENAI_API_KEY\n",
        "  )\n",
        "except userdata.SecretNotFoundError as e:\n",
        "   print(f'''Secret not found\\n\\nThis expects you to create a secret named {openai_api_secret_name} in Colab\\n\\nVisit https://platform.openai.com/api-keys to create an API key\\n\\nStore that in the secrets section on the left side of the notebook (key icon)\\n\\nName the secret {openai_api_secret_name}''')\n",
        "   raise e\n",
        "except userdata.NotebookAccessError as e:\n",
        "  print(f'''You need to grant this notebook access to the {openai_api_secret_name} secret in order for the notebook to access Gemini on your behalf.''')\n",
        "  raise e\n",
        "except Exception as e:\n",
        "  # unknown error\n",
        "  print(f\"There was an unknown error. Ensure you have a secret {openai_api_secret_name} stored in Colab and it's a valid key from https://platform.openai.com/api-keys\")\n",
        "  raise e"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## System Setup"
      ],
      "metadata": {
        "id": "VDvlToWKzTLa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/EmicoBinsfinder/EPOCodeFestProject.git\n",
        "!pip install gradio"
      ],
      "metadata": {
        "id": "efCLY8SmYC9V",
        "outputId": "acd44351-c5b5-4972-e0d6-7646e0c8343a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'EPOCodeFestProject'...\n",
            "remote: Enumerating objects: 77, done.\u001b[K\n",
            "remote: Counting objects: 100% (23/23), done.\u001b[K\n",
            "remote: Compressing objects: 100% (23/23), done.\u001b[K\n",
            "remote: Total 77 (delta 11), reused 0 (delta 0), pack-reused 54\u001b[K\n",
            "Receiving objects: 100% (77/77), 138.42 MiB | 17.98 MiB/s, done.\n",
            "Resolving deltas: 100% (14/14), done.\n",
            "Updating files: 100% (48/48), done.\n",
            "Collecting gradio\n",
            "  Downloading gradio-4.21.0-py3-none-any.whl (17.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.0/17.0 MB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.2.2)\n",
            "Collecting fastapi (from gradio)\n",
            "  Downloading fastapi-0.110.0-py3-none-any.whl (92 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.1/92.1 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.3.2.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gradio-client==0.12.0 (from gradio)\n",
            "  Downloading gradio_client-0.12.0-py3-none-any.whl (310 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.7/310.7 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.27.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.20.3)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.1.3)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.3)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.5)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: numpy~=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.25.2)\n",
            "Collecting orjson~=3.0 (from gradio)\n",
            "  Downloading orjson-3.9.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.5/138.5 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (23.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.5.3)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (9.4.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.6.3)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Collecting python-multipart>=0.0.9 (from gradio)\n",
            "  Downloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.1)\n",
            "Collecting ruff>=0.2.2 (from gradio)\n",
            "  Downloading ruff-0.3.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting tomlkit==0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: typer[all]<1.0,>=0.9 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.9.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.10.0)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.28.0-py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==0.12.0->gradio) (2023.6.0)\n",
            "Collecting websockets<12.0,>=10.0 (from gradio-client==0.12.0->gradio)\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.0.4)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (3.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (3.13.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (4.66.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (4.49.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2023.4)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.16.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer[all]<1.0,>=0.9->gradio) (8.1.7)\n",
            "Collecting colorama<0.5.0,>=0.4.3 (from typer[all]<1.0,>=0.9->gradio)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Collecting shellingham<2.0.0,>=1.3.0 (from typer[all]<1.0,>=0.9->gradio)\n",
            "  Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: rich<14.0.0,>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer[all]<1.0,>=0.9->gradio) (13.7.1)\n",
            "Collecting starlette<0.37.0,>=0.36.3 (from fastapi->gradio)\n",
            "  Downloading starlette-0.36.3-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.33.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.18.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (2.16.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.24.1->gradio) (1.2.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->gradio) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->gradio) (2.0.7)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (0.1.2)\n",
            "Building wheels for collected packages: ffmpy\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpy: filename=ffmpy-0.3.2-py3-none-any.whl size=5584 sha256=b89fad8cdeaeabb29424c47adb12e315b953cb25b211b6276ff50275420206b2\n",
            "  Stored in directory: /root/.cache/pip/wheels/bd/65/9a/671fc6dcde07d4418df0c592f8df512b26d7a0029c2a23dd81\n",
            "Successfully built ffmpy\n",
            "Installing collected packages: pydub, ffmpy, websockets, uvicorn, tomlkit, shellingham, semantic-version, ruff, python-multipart, orjson, colorama, aiofiles, starlette, gradio-client, fastapi, gradio\n",
            "Successfully installed aiofiles-23.2.1 colorama-0.4.6 fastapi-0.110.0 ffmpy-0.3.2 gradio-4.21.0 gradio-client-0.12.0 orjson-3.9.15 pydub-0.25.1 python-multipart-0.0.9 ruff-0.3.2 semantic-version-2.10.0 shellingham-1.5.4 starlette-0.36.3 tomlkit-0.12.0 uvicorn-0.28.0 websockets-11.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#Path to embedding Model\n",
        "Model_Path = '/content/EPOCodeFestProject/TextSimilarityModel'"
      ],
      "metadata": {
        "id": "vC_VUKLF5lwI",
        "outputId": "2034df17-471e-4cc8-c55f-0d68287e2b0b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "########## IMPORTING REQUIRED PYTHON PACKAGES ##########\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "import math\n",
        "import time\n",
        "import csv\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "import string\n",
        "import gradio\n",
        "import os\n",
        "import pprint\n",
        "from elasticsearch import Elasticsearch\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.memory import ElasticsearchChatMessageHistory\n",
        "from uuid import uuid4"
      ],
      "metadata": {
        "id": "7mbvLF34rrp2",
        "outputId": "26bbba59-1715-4f01-bf01-8911e5cfacf5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining embedding generation"
      ],
      "metadata": {
        "id": "n2opssXczXFA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Format embeddings"
      ],
      "metadata": {
        "id": "c8ggBIhn0hPu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mean_pooling(model_output, attention_mask):\n",
        "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
        "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "    return tf.reduce_sum(token_embeddings * input_mask_expanded, 1) / tf.clip_by_value(input_mask_expanded.sum(1), clip_value_min=1e-9, clip_value_max=math.inf)"
      ],
      "metadata": {
        "id": "pIhfU5Vix57o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Function to embed the input text for similarity searching"
      ],
      "metadata": {
        "id": "mcZPHaSd0pch"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Sentence Embedder\n",
        "def sentence_embedder(sentences, model_path):\n",
        "  \"\"\"\n",
        "  Calling the sentence similarity model to generate embeddings on input text.\n",
        "  :param sentences: takes input text in the form of a string\n",
        "  :param model_path: path to the text similarity model\n",
        "  :return returns a (1, 384) embedding of the input text\n",
        "  \"\"\"\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_path) #instantiating the sentence embedder using HuggingFace library\n",
        "  model = AutoModel.from_pretrained(model_path, from_tf=True) #making a model instance\n",
        "  encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
        "  # Compute token embeddings\n",
        "  with torch.no_grad():\n",
        "    model_output = model(**encoded_input)\n",
        "  sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask']) #outputs a (1, 384) tensor representation of input text\n",
        "  return sentence_embeddings"
      ],
      "metadata": {
        "id": "9dAU7mzw4yhv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Saved Embeddings"
      ],
      "metadata": {
        "id": "hTPxCNKc40RM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class_embeddings = pd.read_csv('/content/EPOCodeFestProject/MainClassEmbeddings.csv')"
      ],
      "metadata": {
        "id": "3J6PxzA7869m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Sentence Embedding Preparation Function\n",
        "def convert_saved_embeddings(embedding_string):\n",
        "    \"\"\"\n",
        "    Preparing pre-computed embeddings for use for comparison with new abstract embeddings .\n",
        "    Pre-computed embeddings are saved as tensors in string format so need to be converted back to numpy arrays in order to calculate cosine similarity.\n",
        "    :param embedding_string:\n",
        "    :return: Should be a single tensor with dims (,384) in string formate\n",
        "    \"\"\"\n",
        "    embedding = embedding_string.replace('(', '')\n",
        "    embedding = embedding.replace(')', '')\n",
        "    embedding = embedding.replace('[', '')\n",
        "    embedding = embedding.replace(']', '')\n",
        "    embedding = embedding.replace('tensor', '')\n",
        "    embedding = embedding.replace(' ', '')\n",
        "    embedding = embedding.split(',')\n",
        "    embedding = [float(x) for x in embedding]\n",
        "    embedding = np.array(embedding)\n",
        "    embedding = np.expand_dims(embedding, axis=0)\n",
        "    embedding = torch.from_numpy(embedding)\n",
        "    return embedding"
      ],
      "metadata": {
        "id": "tO93a3K8v_if"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Clean User Input"
      ],
      "metadata": {
        "id": "WXZDBKas8Uu3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_stopwords = stopwords.words('english') # Making sure to only use English stopwords\n",
        "extra_stopwords = ['ii', 'iii'] # Can add extra stopwords to be removed from dataset/input abstracts\n",
        "all_stopwords.extend(extra_stopwords)\n",
        "\n",
        "def clean_data(input, type='Dataframe'):\n",
        "    if type == 'Dataframe':\n",
        "        cleaneddf = pd.DataFrame(columns=['Class', 'Description'])\n",
        "        for i in range(0, len(input)):\n",
        "            row_list = input.loc[i, :].values.flatten().tolist()\n",
        "            noNaN_row = [x for x in row_list if str(x) != 'nan']\n",
        "            listrow = []\n",
        "            if len(noNaN_row) > 0:\n",
        "                row = noNaN_row[:-1]\n",
        "                row = [x.strip() for x in row]\n",
        "                row = (\" \").join(row)\n",
        "                text_tokens = word_tokenize(row)  # splits abstracts into individual tokens to allow removal of stopwords by list comprehension\n",
        "                Stopword_Filtered_List = [word for word in text_tokens if not word in all_stopwords]  # removes stopwords\n",
        "                row = (\" \").join(Stopword_Filtered_List)  # returns abstract to string form\n",
        "                removechars = ['[', ']', '{', '}', ';', '(', ')', ',', '.', ':', '/', '-', '#', '?', '@', '£', '$']\n",
        "                for char in removechars:\n",
        "                    row = list(map(lambda x: x.replace(char, ''), row))\n",
        "\n",
        "                row = ''.join(row)\n",
        "                wnum = row.split(' ')\n",
        "                wnum = [x.lower() for x in wnum]\n",
        "                #remove duplicate words\n",
        "                wnum = list(dict.fromkeys(wnum))\n",
        "                #removing numbers\n",
        "                wonum = []\n",
        "                for x in wnum:\n",
        "                    xv = list(x)\n",
        "                    xv = [i.isnumeric() for i in xv]\n",
        "                    if True in xv:\n",
        "                        continue\n",
        "                    else:\n",
        "                        wonum.append(x)\n",
        "                row = ' '.join(wonum)\n",
        "                l = [noNaN_row[-1], row]\n",
        "                cleaneddf.loc[len(cleaneddf)] = l\n",
        "        cleaneddf = cleaneddf.drop_duplicates(subset=['Description'])\n",
        "        cleaneddf.to_csv('E:/Users/eeo21/Startup/CPC_Classifications_List/additionalcleanedclasses.csv', index=False)\n",
        "        return cleaneddf\n",
        "\n",
        "    elif type == 'String':\n",
        "        text_tokens = word_tokenize(input)  # splits abstracts into individual tokens to allow removal of stopwords by list comprehension\n",
        "        Stopword_Filtered_List = [word for word in text_tokens if not word in all_stopwords]  # removes stopwords\n",
        "        row = (\" \").join(Stopword_Filtered_List)  # returns abstract to string form\n",
        "        removechars = ['[', ']', '{', '}', ';', '(', ')', ',', '.', ':', '/', '-', '#', '?', '@', '£', '$']\n",
        "        for char in removechars:\n",
        "            row = list(map(lambda x: x.replace(char, ''), row))\n",
        "        row = ''.join(row)\n",
        "        wnum = row.split(' ')\n",
        "        wnum = [x.lower() for x in wnum]\n",
        "        # remove duplicate words\n",
        "        wnum = list(dict.fromkeys(wnum))\n",
        "        # removing numbers\n",
        "        wonum = []\n",
        "        for x in wnum:\n",
        "            xv = list(x)\n",
        "            xv = [i.isnumeric() for i in xv]\n",
        "            if True in xv:\n",
        "                continue\n",
        "            else:\n",
        "                wonum.append(x)\n",
        "        row = ' '.join(wonum)\n",
        "        return row"
      ],
      "metadata": {
        "id": "Gt7Gfums8Wnh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Function for CPC Class Prediction"
      ],
      "metadata": {
        "id": "D2hTFnal5__M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def broad_scope_class_predictor(class_embeddings, abstract_embedding, N=10, Sensitivity='Medium'):\n",
        "    predictions = pd.DataFrame(columns=['Class Name', 'Score', 'Description'])\n",
        "    for i in range(len(class_embeddings)):\n",
        "        class_name = class_embeddings.iloc[i, 0]\n",
        "        embedding = class_embeddings.iloc[i, 2]\n",
        "        description = class_embeddings.iloc[i, 1]\n",
        "        embedding = convert_saved_embeddings(embedding)\n",
        "        abstract_embedding = abstract_embedding.numpy()\n",
        "        abstract_embedding = torch.from_numpy(abstract_embedding)\n",
        "        cos = torch.nn.CosineSimilarity(dim=1)\n",
        "        score = cos(abstract_embedding, embedding).numpy().tolist()\n",
        "        result = [class_name, score[0], description]\n",
        "        predictions.loc[len(predictions)] = result\n",
        "\n",
        "    HighestSimilarityDF = predictions.nlargest(N, ['Score'])\n",
        "    HighestSimilarity = HighestSimilarityDF['Class Name'].tolist()\n",
        "    Description = HighestSimilarityDF['Description'].tolist()\n",
        "    HighestSimilarityClass = [x for x in HighestSimilarity]\n",
        "\n",
        "    Links = [f'https://www.patbase.com/classSnapshot/public/?class={x}&system=CPC' for x in HighestSimilarityClass]\n",
        "\n",
        "    HighestSimilarity = pd.DataFrame({'Class':HighestSimilarity, 'Links':Links, 'Description':Description})\n",
        "    Description = ' '.join(Description[:5])\n",
        "\n",
        "    return HighestSimilarity, Description\n",
        "\n",
        "def classifier(userin):\n",
        "    cleaned_input = clean_data(userin, type='String')\n",
        "    input_embedding = sentence_embedder(cleaned_input, Model_Path)\n",
        "\n",
        "    Number = 10\n",
        "    broad_scope_predictions, descriptions = broad_scope_class_predictor(class_embeddings, input_embedding, Number, Sensitivity='High')\n",
        "\n",
        "    return broad_scope_predictions, descriptions\n"
      ],
      "metadata": {
        "id": "SyXfZcrB6GxJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Prompt Creation"
      ],
      "metadata": {
        "id": "7giOE3Tw6ziR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Gradio App"
      ],
      "metadata": {
        "id": "RI26vOs9AFaG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chatbot(input):\n",
        "  predictions, CPC_Descriptions = classifier(input)\n",
        "\n",
        "  class_links = []\n",
        "  for i in range(len(predictions)):\n",
        "    class_links.append(\"[{}]({})\".format(predictions['Class'][i], predictions['Links'][i]))\n",
        "\n",
        "  links = '\\n'.join(class_links)\n",
        "\n",
        "  Prompt = f'''Based on the description below and the patent claim set,\n",
        "generate a comprehensive list of relevant tags/keywords related to the claims where the\n",
        "tags fall into one of the following categories:\n",
        "\n",
        "1. Product: What is the general product area\n",
        "2. Function: What is the function of the invention\n",
        "3. Component: What components does the invention comprise of\n",
        "4. Invention type: Is it for example a method claim, apparatus claim, method-of-use\n",
        "\n",
        "For each tag, also return which of the 4 categories it belongs to in the following format:\n",
        "\n",
        "Attempt to extract as many tags/keywords from the claims as possible.\n",
        "\n",
        "{{Tag: Category}}\n",
        "\n",
        "Description:\n",
        "{CPC_Descriptions}\n",
        "\n",
        "Claims:\n",
        "{input}\n",
        "'''\n",
        "\n",
        "  completion = client.chat.completions.create(\n",
        "  model=\"gpt-4-0125-preview\",\n",
        "  messages=[\n",
        "  {\"role\": \"user\", \"content\": f'Your function is that of a bot optimised for summarising patent text. Answer the following query as accurately as possible based on your function {Prompt}'}\n",
        "  ]\n",
        "  )\n",
        "\n",
        "  response = completion.choices[0].message.content\n",
        "\n",
        "  response = '\\n'.join([response, f\"{'#'*120} \\n\"])\n",
        "  response = '\\n'.join([response, f\"Classes USED TO GENERATE RESPONSE:\\n {links}\"])\n",
        "\n",
        "  return response\n",
        "\n",
        "inputs = gradio.Textbox(lines=7, label=\"Generate tags and CPC classifications based on patent claims\")\n",
        "outputs = gradio.Textbox(label=\"Reply\")\n",
        "\n",
        "gradio.Interface(fn=chatbot, inputs=inputs, outputs=outputs, title=\"Patent Tagging Prototype\",\n",
        "             theme=\"compact\").launch(share=True, debug=True)"
      ],
      "metadata": {
        "id": "UUiITGnU3I0W",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 862
        },
        "outputId": "3442ea27-b728-442f-c482-423cdd004c81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gradio/blocks.py:565: UserWarning: Cannot load compact. Caught Exception: 404 Client Error: Not Found for url: https://huggingface.co/api/spaces/compact (Request ID: Root=1-65ef4fbc-411c804f4d2bdbb5541241a4;48588fc6-1d8b-42fa-b807-2c63b58185e0)\n",
            "\n",
            "Sorry, we can't find the page you are looking for.\n",
            "  warnings.warn(f\"Cannot load {theme}. Caught Exception: {str(e)}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://7d9fc91aee348244e8.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://7d9fc91aee348244e8.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All TF 2.0 model weights were used when initializing BertModel.\n",
            "\n",
            "All the weights of BertModel were initialized from the TF 2.0 model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n"
          ]
        }
      ]
    }
  ]
}