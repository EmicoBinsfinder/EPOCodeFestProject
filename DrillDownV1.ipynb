{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPuR+HeQpi/rinvviOxpgLC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EmicoBinsfinder/EPOCodeFestProject/blob/main/DrillDownV1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "AwZLm-jWYBaO",
        "cellView": "form",
        "outputId": "a3f0360f-e1bb-40ff-e917-5528e396fc0d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m257.5/257.5 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.7/52.7 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "#@title Configure OpenAI API key\n",
        "\n",
        "# access your OpenAI API key\n",
        "\n",
        "# installing llmx first isn't necessary but avoids a confusing error when installing openai\n",
        "!pip install -q llmx\n",
        "!pip install -q openai\n",
        "from openai import OpenAI\n",
        "import google.generativeai as genai\n",
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "openai_api_secret_name = 'Test'\n",
        "## @param {type: \"string\"}\n",
        "\n",
        "try:\n",
        "  OPENAI_API_KEY=userdata.get(openai_api_secret_name)\n",
        "  OpenAIclient = OpenAI(\n",
        "    api_key=OPENAI_API_KEY\n",
        "  )\n",
        "except userdata.SecretNotFoundError as e:\n",
        "   print(f'''Secret not found\\n\\nThis expects you to create a secret named {openai_api_secret_name} in Colab\\n\\nVisit https://platform.openai.com/api-keys to create an API key\\n\\nStore that in the secrets section on the left side of the notebook (key icon)\\n\\nName the secret {openai_api_secret_name}''')\n",
        "   raise e\n",
        "except userdata.NotebookAccessError as e:\n",
        "  print(f'''You need to grant this notebook access to the {openai_api_secret_name} secret in order for the notebook to access Gemini on your behalf.''')\n",
        "  raise e\n",
        "except Exception as e:\n",
        "  # unknown error\n",
        "  print(f\"There was an unknown error. Ensure you have a secret {openai_api_secret_name} stored in Colab and it's a valid key from https://platform.openai.com/api-keys\")\n",
        "  raise e"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### System Setup"
      ],
      "metadata": {
        "id": "VDvlToWKzTLa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install gradio\n",
        "# !pip install elasticsearch\n",
        "# !pip install langchain"
      ],
      "metadata": {
        "id": "efCLY8SmYC9V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########## IMPORTING REQUIRED PYTHON PACKAGES ##########\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "import math\n",
        "import time\n",
        "import csv\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "import string\n",
        "import gradio\n",
        "import os\n",
        "import pprint\n",
        "from elasticsearch import Elasticsearch\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.memory import ElasticsearchChatMessageHistory\n",
        "from uuid import uuid4\n",
        "import os, sys\n",
        "import json, csv"
      ],
      "metadata": {
        "id": "7mbvLF34rrp2",
        "outputId": "f03dec5a-aeaa-4bb2-c16d-e8bb5da3716d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get my os environment\n",
        "os.environ['ELASTICSEARCH_PASSWORD'] = 'l0ng-r4nd0m-p@ssw0rd'\n",
        "pwd = os.environ[\"ELASTICSEARCH_PASSWORD\"]\n",
        "\n",
        "# Password for the 'elastic' user generated by Elasticsearch\n",
        "ELASTIC_PASSWORD = pwd\n",
        "\n",
        "# Found in the 'Manage Deployment' page\n",
        "CLOUD_ID = \"http://AnkarDev-Elasticsearch-1891076460.eu-west-2.elb.amazonaws.com:9200\"\n",
        "\n",
        "# Create the client instance\n",
        "client = Elasticsearch(\n",
        "    CLOUD_ID,\n",
        "    basic_auth=(\"eogbomo\", ELASTIC_PASSWORD),\n",
        "    verify_certs=False\n",
        ")"
      ],
      "metadata": {
        "id": "L8N-BJExLO6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Gradio App"
      ],
      "metadata": {
        "id": "RI26vOs9AFaG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_query1 = {\"size\": 1,\"sort\": [{\"publicationDate\": {\"order\": \"desc\"}}],\"query\": {\"bool\": {\"must\": [{\"match\": {\"applicants\": \"apple\"}}]}}}\n",
        "example_query2 = {\"size\": 1,\"sort\": [{\"publicationDate\": {\"order\": \"desc\"}}],\"query\": {\"bool\": {\"must\": [{\"match\": {\"applicants\": \"apple\"}}]}}}"
      ],
      "metadata": {
        "id": "J6Y_FRWeQ478"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loadprevresponses():\n",
        "  try:\n",
        "    with open ('responses.json', 'r+') as file:\n",
        "      try:\n",
        "        data = json.load(file)\n",
        "      except:\n",
        "        print('Error loading responses')\n",
        "        data = {}\n",
        "  except FileNotFoundError:\n",
        "    with open ('responses.json', 'w') as file:\n",
        "      data = {}\n",
        "      json.dump(data, file)\n",
        "  return data\n",
        "\n",
        "def saveresponse(input):\n",
        "  history = loadprevresponses()\n",
        "\n",
        "  history[f'Input{len(history)+1}'] = input\n",
        "  with open ('responses.json', 'r+') as file:\n",
        "    json.dump(history, file)\n",
        ""
      ],
      "metadata": {
        "id": "pxLEKRmTpClm",
        "outputId": "370eff37-4d2e-418a-9e1b-7167727d90c5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Input1': 'What is Valeo\"s latest patent about cameras?', 'Input2': 'What is Valeo\"s latest patent about cameras?'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def DrillDown(input):\n",
        "\n",
        "  history = loadprevresponses()\n",
        "  saveresponse(input)\n",
        "\n",
        "  prompt = \"\"\"You are an expert in translating natural language queries about patents into ElasticSearch Queries.\n",
        "    Given a user input and the provided dictionary of user input history, create an Elasticsearch query enabling the user to return as many relevant patents as possible when querying in Elastic\n",
        "\n",
        "    input: {input}\n",
        "\n",
        "    history: {history}\n",
        "\n",
        "    \"\"\".format(input=input, history=history)\n",
        "\n",
        "  additional_prompt=\"\"\"\n",
        "    Instructions:\n",
        "    1. Generate Elasticsearch queries based on the provided natural language queries.\n",
        "    2. Only use fields present in the mapping. If the user is asking about a field that is not in the mapping ignore it.\n",
        "    3. Ensure that the generated queries follow Elasticsearch's query DSL syntax and structure.\n",
        "    4. You can correct or reformulate the user's query if it has errors.\n",
        "    5. Return all fields in your response when applicable.\n",
        "    6. Make sure that the query only performs full text search when applicable i.e. don't use keyword search\n",
        "    7. When returning the json portion of the answer, compress the json output removing spaces. Remove any mention of json in the output or triple backtick sand make sure that it's valid.\n",
        "    8. Ensure that as many aspects of the user input\n",
        "    9. Also provide a short paragraph explaning why the ElasticSearch query was created as it was.\n",
        "\n",
        "    Examples of expected behavior:\n",
        "    Natural Language Query: \"What is the title of the most recent Apple patent\"\n",
        "    Expected Elasticsearch Query:\n",
        "    {\n",
        "      \"size\": 1,\n",
        "      \"sort\": [\n",
        "        {\n",
        "          \"publicationDate\": {\n",
        "            \"order\": \"desc\"\n",
        "          }\n",
        "        }\n",
        "      ],\n",
        "      \"query\": {\n",
        "        \"bool\": {\n",
        "          \"must\": [\n",
        "            {\n",
        "              \"match\": {\n",
        "                \"applicants\": \"apple\"\n",
        "              }\n",
        "            }\n",
        "          ]\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "\n",
        "    Natural Language Query: \"What are the most recent methods to deal with cell group failure?\"\n",
        "    Expected Elasticsearch Query:\n",
        "    {\n",
        "      \"query\": {\n",
        "        \"bool\": {\n",
        "          \"must\": [\n",
        "            {\n",
        "              \"bool\": {\n",
        "                \"should\": [\n",
        "                  {\n",
        "                    \"match\": {\n",
        "                      \"patentTitle\": \"cell group failure\"\n",
        "                    }\n",
        "                  },\n",
        "                  {\n",
        "                    \"match\": {\n",
        "                      \"patentAbstract\": \"cell group failure\"\n",
        "                    }\n",
        "                  },\n",
        "                  {\n",
        "                    \"match\": {\n",
        "                      \"claims.claimText\": \"cell group failure\"\n",
        "                    }\n",
        "                  },\n",
        "                  {\n",
        "                    \"match\": {\n",
        "                      \"patentDescription\": \"cell group failure\"\n",
        "                    }\n",
        "                  }\n",
        "                ]\n",
        "              }\n",
        "            }\n",
        "          ],\n",
        "          \"filter\": [\n",
        "            {\n",
        "              \"range\": {\n",
        "                \"publicationDate\": {\n",
        "                  \"gte\": \"now-5y/d\"\n",
        "                }\n",
        "              }\n",
        "            }\n",
        "          ]\n",
        "        }\n",
        "      },\n",
        "      \"_source\": [\"*\"]\n",
        "    }\"\"\"\n",
        "  prompt += additional_prompt\n",
        "\n",
        "  completion = OpenAIclient.chat.completions.create(\n",
        "  model=\"gpt-4-0125-preview\",\n",
        "  messages=[\n",
        "  {\"role\": \"user\", \"content\": f'Your function is that of a bot optimised for summarising patent text. Answer the following query as accurately as possible based on your function {prompt}'}\n",
        "  ]\n",
        "  )\n",
        "  response = completion.choices[0].message.content\n",
        "\n",
        "  response = '\\n'.join([response, f\"{'#'*120} \\n\"])\n",
        "  response = '\\n'.join([response, f\"History USED TO GENERATE RESPONSE:\\n {history}\"])\n",
        "\n",
        "  return response\n",
        "\n",
        "inputs = gradio.Textbox(lines=7, label=\"Generate Queries for use with Elastic Search, allowing for search refinement\")\n",
        "outputs = gradio.Textbox(label=\"Reply\")\n",
        "\n",
        "gradio.Interface(fn=DrillDown, inputs=inputs, outputs=outputs, title=\"Patent DrillDown Prototype\",\n",
        "             theme=\"compact\").launch(share=True, debug=True)"
      ],
      "metadata": {
        "id": "UUiITGnU3I0W",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 755
        },
        "outputId": "f6452e00-87bd-4748-df80-a6d360e73925"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gradio/blocks.py:565: UserWarning: Cannot load compact. Caught Exception: 404 Client Error: Not Found for url: https://huggingface.co/api/spaces/compact (Request ID: Root=1-65f9caaa-367f5254620caafa69d07340;8d202875-0ffa-4c71-b598-76afbb86a8b3)\n",
            "\n",
            "Sorry, we can't find the page you are looking for.\n",
            "  warnings.warn(f\"Cannot load {theme}. Caught Exception: {str(e)}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://fb3b19fd5648fb2b30.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://fb3b19fd5648fb2b30.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Input1': \"what are valeo's patents on cameras\"}\n",
            "{'Input1': \"what are valeo's patents on cameras\", 'Input2': \"what are valeo's patents on lidar\"}\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7861 <> https://fb3b19fd5648fb2b30.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get search response"
      ],
      "metadata": {
        "id": "9jaaJdkDmYOl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = {\n",
        "\"query\":{\n",
        "\"bool\":{\n",
        "\"must\":[\n",
        "{\n",
        "\"match\":{\n",
        "\"applicants\":\"valeo\"\n",
        "}\n",
        "},\n",
        "{\n",
        "\"bool\":{\n",
        "\"should\":[\n",
        "{\n",
        "\"match\":{\n",
        "\"patentTitle\":\"lidar\"\n",
        "}\n",
        "},\n",
        "{\n",
        "\"match\":{\n",
        "\"patentAbstract\":\"lidar\"\n",
        "}\n",
        "},\n",
        "{\n",
        "\"match\":{\n",
        "\"claims.claimText\":\"lidar\"\n",
        "}\n",
        "},\n",
        "{\n",
        "\"match\":{\n",
        "\"patentDescription\":\"lidar\"\n",
        "}\n",
        "}\n",
        "]\n",
        "}\n",
        "}\n",
        "]\n",
        "}\n",
        "},\n",
        "\"_source\":[\"*\"]\n",
        "}\n"
      ],
      "metadata": {
        "id": "otheqsp3mfc3"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resp = client.search(index=\"patents\",\n",
        "                     body=query)\n",
        "\n",
        "print(resp['hits']['hits'][0])"
      ],
      "metadata": {
        "id": "Q8hePjn5NQ4E",
        "outputId": "277a6ff1-99db-4fca-c769-bbb80df3ab2c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'_index': 'patents_v0', '_id': 'EP_EP23156734A1', '_score': 11.503514, '_source': {'patentId': 'EP23156734A1', 'patentOffice': 'EP', 'patentNumber': '4235212', 'kindCode': 'A1', 'publicationDate': '20230830', 'applicationNumber': '23156734.8', 'applicationDate': '20230215', 'ipcrClassification': [{'section': 'G', 'ipcrClass': '01', 'ipcrSubClass': 'S', 'ipcrClassification': 'G01S'}, {'section': 'G', 'ipcrClass': '08', 'ipcrSubClass': 'G', 'ipcrClassification': 'G08G'}, {'section': 'B', 'ipcrClass': '60', 'ipcrSubClass': 'W', 'ipcrClassification': 'B60W'}], 'title': 'LENGTH ESTIMATION OF A VEHICLE USING LIDAR POINT CLOUDS', 'applicants': ['Valeo Schalter und Sensoren GmbH'], 'inventors': ['Nasir, Umair', 'Amarendra, Bharath'], 'patentAbstract': 'According to a method for estimating a length of a vehicle (1) driving in front of an ego-vehicle (8), a sensor dataset is generated by means of a lidar system (11) of the ego-vehicle (8), wherein the vehicle (1) is within a field of view (10) of the lidar system (11). A first distance (X0) between the ego-vehicle (8) and a rear end (2) of the vehicle (1) is determined depending on the sensor dataset. A second distance (X3) between the ego-vehicle (8) and a reference component, which is at least a part of a front wheel (5) of the vehicle (1) or an outer part of an underbody (6) of the vehicle (1), is determined depending on the sensor dataset, and the length of the vehicle (1) is estimated depending on the first distance (X0) and the second distance (X3).', 'patentDescription': [{'heading': 'NO HEADING', 'paragraphs': [{'paragraphId': 'p0001', 'paragraphText': 'The present invention is directed to a method for estimating a length of a vehicle driving in front of an ego-vehicle, wherein at least one sensor dataset is generated by means of a lidar system of the ego-vehicle, wherein the vehicle is within a field of view of the lidar system. A first distance between the ego-vehicle and a rear end of the vehicle is determined depending on the at least one sensor dataset. The invention is further directed to a method for guiding an ego-vehicle at least one in part automatically, to a vehicle length estimation system, to an electronic vehicle guidance system and to a motor vehicle.'}, {'paragraphId': 'p0002', 'paragraphText': 'Many vehicles are equipped with advanced driver assistance systems, ADAS, or other electronic vehicle guidance systems for guiding the vehicle automatically or in part automatically. Therein, the electronic vehicle guidance system may, in particular, affect or partially or completely take over the longitudinal and/or lateral control of the ego-vehicle. In traffic situations, information about the environment and objects in the environment, in particular further vehicles, is vital to implement a safe guidance of the ego-vehicle. In particular, the outer dimensions of other vehicles in the environment, for example the length of those vehicles, may be an important information for the electronic vehicle guidance system. For example, if an automatic or semi-automatic overtaking maneuver is planned by the electronic vehicle guidance system, the length of the vehicle to be overtaken may be taken into account to ensure an efficient and safe maneuver.'}, {'paragraphId': 'p0003', 'paragraphText': 'In scenarios where the ego-vehicle and the other vehicle are driving along different lanes more or less next to each other, the length of the vehicle can be estimated for example by means of side cameras of the ego-vehicle. However, in scenarios where the vehicle is driving in front of the ego-vehicle, this is in general not possible in the same way.'}, {'paragraphId': 'p0004', 'paragraphText': \"Document WO 2017/080930 A1 is directed to a vehicle length determination system of a vehicle, which is configured to determine a length of another vehicle in the vehicle's vicinity. A detection unit is adapted to detect a first distance between the vehicle and a front side or rear side of the other vehicle and to detect a second distance between the vehicle and a side mirror of the other vehicle. A control unit is adapted to calculate the length of the other vehicle depending on the determined first and second distance. Consequently, the length cannot be determined exactly in this way. If the first distance is determined with respect to the rear side, the length can only be determined up to a distance of the side mirror to the front side of the other vehicle. On the other hand, if the first distance is determined according to the front side of the other vehicle, the length may only be estimated up to the distance of the rear side to the side mirror. Since the side mirrors are commonly not mounted particularly close to the front side or the rear side of the other vehicle, a relatively large uncertainty for estimating the length results.\"}, {'paragraphId': 'p0005', 'paragraphText': 'It is an objective of the present invention, to estimate the length of a vehicle driving in front of an ego-vehicle more accurately or with less uncertainty, respectively.'}, {'paragraphId': 'p0006', 'paragraphText': 'This objective is achieved by the respective subject matter of the independent claims. Further implementations and preferred embodiments are subject matter of the dependent claims.'}, {'paragraphId': 'p0007', 'paragraphText': 'The invention is based on the idea to use a lidar system of the ego-vehicle to detect a distance between the ego-vehicle and a reference component of the vehicle, which is a front wheel, a part of the front wheel or an outer part of an underbody of the vehicle. The length of the vehicle is then estimated depending on that distance and a distance between the ego-vehicle and the rear end of the vehicle.'}, {'paragraphId': 'p0008', 'paragraphText': 'According to the invention a method for estimating a length of a vehicle driving in front of an ego-vehicle is provided. At least one sensor dataset is generated by means of a lidar system of the ego-vehicle, wherein the vehicle driving in front of the ego-vehicle is within a field of view of the lidar system. A first distance between the ego-vehicle and a rear end of the vehicle is determined depending on the at least one sensor dataset, in particular by means of at least one computing unit of the ego-vehicle. A second distance between the ego-vehicle and a reference component of the vehicle is determined depending on the at least one sensor dataset, in particular by means of the at least one computing unit. The reference component of the vehicle is at least a part of a front wheel or, in other words, the reference component is a front wheel or a part of the front wheel. Alternatively, the reference component of the vehicle is an outer part of an underbody of the vehicle. The length of the vehicle is estimated depending on the first distance and the second distance, in particular depending on a difference between the first distance and the second distance, in particular by means of the at least one computing unit.'}, {'paragraphId': 'p0009', 'paragraphText': 'That the vehicle is driving in front of the ego-vehicle may for example be understood such as that the ego-vehicle and the vehicle are both driving in a traffic situation on the same road and, if the road has more than one lane, on the same lane of the road. In particular, the vehicle is driving directly in front of the ego-vehicle or, in other words, there is no further vehicle present between the vehicle and the ego-vehicle on the same lane, which could occlude the vehicle for the lidar system.'}, {'paragraphId': 'p0010', 'paragraphText': 'An environmental sensor system can be understood as a sensor system, which is able to generate sensor data or sensor signals, which depict, represent or image an environment of the environmental sensor system. In particular, the ability to capture or detect electromagnetic or other signals from the environment, cannot be considered a sufficient condition for a sensor system being an environmental sensor system. For example, cameras, lidar systems, radar systems or ultrasonic sensor systems may be considered as environmental sensor systems.'}, {'paragraphId': 'p0011', 'paragraphText': 'A known design of lidar systems are so-called laser scanners, in which a laser beam is deflected by means of a deflection unit so that different deflection angles of the laser beam may be realized. The deflection unit may, for example, contain a rotatably mounted mirror. Alternatively, the deflection unit may include a mirror element with a tiltable and/or pivotable surface. The mirror element may, for example, be configured as a micro-electromechanical system, MEMS. In the environment, the emitted laser beams can be partially reflected, and the reflected portions may in turn hit the laser scanner, in particular the deflection unit, which may direct them to a detector unit of the laser scanner comprising the at least one optical detector. In particular, each optical detector of the detector unit generates an associated detector signal based on the portions detected by the respective optical detector. Based on the spatial arrangement of the respective optical detector, together with the current position of the deflection unit, in particular its rotational position or its tilting and/or pivoting position, it is thus possible conclude the direction of incidence of the detected reflected components of light. A processing unit or an evaluation unit of the laser scanner may, for example, perform a time-of-flight measurement to determine a radial distance of the reflecting object. Alternatively or additionally, a method, according to which a phase difference between the emitted and detected light is evaluated, may be used to determine the distance.'}, {'paragraphId': 'p0012', 'paragraphText': 'A point cloud may be understood as a set of points, wherein each point is characterized by respective coordinates in a two-dimensional or three-dimensional coordinate system. In case of a three-dimensional point cloud, the three-dimensional coordinates may, for example, be determined by the direction of incidence of the reflected components of light and the corresponding time-of-flight or radial distance measured for this respective point. In other words, the three-dimensional coordinate system may be a three-dimensional polar coordinate system. However, the information may also be pre-processed to provide three-dimensional Cartesian coordinates for each of the points. In general, the points of a point cloud may be given in an orderless or unsorted manner, in contrast to, for example, a camera image. In addition to the spatial information, namely the two-dimensional or three-dimensional coordinates, the point cloud may also store additional information or measurement values for the individual points such as an echo pulse width, EPW, of the respective sensor signal.'}, {'paragraphId': 'p0013', 'paragraphText': 'The lidar system is, in particular, mounted on the ego-vehicle such that the field of view is oriented at least in part in forward driving direction of the ego-vehicle. For example, the lidar system may be mounted at a front side or front end of the ego-vehicle, for example at a bumper, at a front end of a hood or a cooler of the ego-vehicle or at another suitable position, which allows to detect objects in front of the ego-vehicle, which are relatively close to the road surface, such as the wheel or the underbody of the vehicle.'}, {'paragraphId': 'p0014', 'paragraphText': 'The at least one computing unit of the ego-vehicle may for example be comprise one or more electronic control units, ECUs, of the ego-vehicle or other computing devices of the ego-vehicle. The lidar system may comprise the at least one computing unit or parts of the at least one computing unit or the at least one computing unit may be separated from the lidar system.'}, {'paragraphId': 'p0015', 'paragraphText': 'In particular, the at least one computing unit and/or the lidar system may be part of an electronic vehicle guidance system of the ego-vehicle.'}, {'paragraphId': 'p0016', 'paragraphText': 'The underbody of the vehicle, which may also be denoted as under-chassis of the vehicle may be considered as a part of the body or chassis of the vehicle, which faces the road surface or, in certain regions, the wheels of the vehicle. For example, the underbody may be considered as the entirety of all components of the vehicle, which are arranged outside of the vehicle and face the road surface or the wheels of the vehicle. The underbody may for example comprise components or parts of components such as an axle, in particular a front axle, of the vehicle, a suspension system of the vehicle, a transverse link of the vehicle et cetera. Consequently, in case the reference component of the vehicle is an outer part of the underbody of the vehicle, the reference component is, in particular, a vehicle part that protrudes downward from the vehicle towards the road surface.'}, {'paragraphId': 'p0017', 'paragraphText': 'Here and in the following, a wheel of a vehicle may be understood as a complete wheel comprising in particular a rim and a tire of the wheel. In case the reference component is at least a part of the front wheel, it may therefore be for example a part of the rim and/or the tire.'}, {'paragraphId': 'p0018', 'paragraphText': 'To determine the first and the second distance, respectively, the at least one computing unit, in particular, detects the rear end of the vehicle and detects the reference component, respectively, based on the at least one sensor dataset and then calculates the distance based on the detected reference component and the rear end.'}, {'paragraphId': 'p0019', 'paragraphText': 'In particular, the at least one sensor dataset may comprise one or more point clouds, which contains a plurality of scan points. To detect the rear end and the reference component, respectively, the at least one computing unit may, in particular, identify those scan points of the plurality of scan points, which correspond to or apparently correspond to the rear end of the vehicle and the reference component, respectively. To this end, the at least one computing unit may apply one or more filter algorithms or rules or plausibility checks, which may for example concern the distance of the respective scan points from the ego-vehicle, their height above the road surface, the angle of incidence of the correspondingly reflected light, the reflectivity of the reflecting target in the environment and so forth.'}, {'paragraphId': 'p0020', 'paragraphText': 'In particular, the at least one computing unit may detect to or more point clusters of scan points of the plurality of scan points based on the at least one sensor dataset and determine, which of the point clusters correspond to the rear end and the reference component, respectively. For example, the rear end may correspond to the point cluster with the least distance from the ego-vehicle. The reference component may for example give rise to a point cluster at a predefined typical distance range.'}, {'paragraphId': 'p0021', 'paragraphText': 'In particular, the second distance is greater than a third distance between the ego-vehicle and a side mirror of the vehicle.'}, {'paragraphId': 'p0022', 'paragraphText': 'The difference between the first and the second distance, in particular the second distance minus the first distance, may be considered as an approximate length of the vehicle. Even though the reference component is not necessarily located at the front end of the vehicle, this approximate length may already be sufficient for certain applications. Furthermore, an offset value may be added to the difference between the first and the second distance to get an improved estimation. The exact length of the vehicle may, in particular be considered as a distance between the rear end and the front end of the vehicle.'}, {'paragraphId': 'p0023', 'paragraphText': 'By considering the front wheel or a part of the front wheel or an outer part of the underbody of the vehicle as the reference component, an improved estimation of the length of the vehicle is possible, in particular in case the second distance is greater than the third distance and, consequently, the reference component is closer to the front end of the vehicle than the side mirrors of the vehicle.'}, {'paragraphId': 'p0024', 'paragraphText': 'The invention exploits the fact that a lidar system, which is appropriately mounted on the ego-vehicle, is able to detect components underneath the body of the vehicle or between the road surface and the vehicle, respectively, such as the underbody or the wheels of the vehicle. In combination with knowledge or assumptions regarding the position of the reference component with respect to the front end of the vehicle, a reliable estimation of the length is achievable.'}, {'paragraphId': 'p0025', 'paragraphText': 'According to several implementations of the method for estimating the length of the vehicle, the length of the vehicle is estimated, in particular by the at least one computing unit, as a sum of an offset value and the difference between the first distance and the second distance.'}, {'paragraphId': 'p0026', 'paragraphText': 'The offset value compensates the distance of the front end of the vehicle to the reference component and is, in particular, a predefined or predetermined offset value. For example, the offset value may be a constant default value for all vehicles or may be selected from a plurality of default values for different types of vehicles, such as passenger cars, trucks, vans, et cetera. However, a more detailed distinction is also possible. The information regarding the type of the vehicle may be obtained for example according to a known method for object detection, for example camera-based object detection or lidar-based object detection or radar-based object detection.'}, {'paragraphId': 'p0027', 'paragraphText': 'Furthermore, also the difference between the first distance and the second distance may be used to estimate the offset value. For example, a database may be provided, which stores corresponding offset values associated to the difference between the first and the second distance for a plurality of existing vehicles or vehicle types.'}, {'paragraphId': 'p0028', 'paragraphText': 'In other words, according to several implementations, the offset value is selected, in particular by means of the at least one computing unit, depending on the difference between the first distance and the second distance from a database, which is, in particular, stored on a memory device of the ego-vehicle, for example the at least one computing unit, or on a memory device of an external computing unit, such as a server or cloud system.'}, {'paragraphId': 'p0029', 'paragraphText': 'According to several implementations, a model or type of the vehicle is determined, in particular by the at least one computing unit, depending on the at least one sensor dataset and/or depending on at least one camera image, which is generated by means of at least one camera of the ego-vehicle and which depicts the vehicle. The offset value is determined, for example selected from the database, depending on the model or type of the vehicle.'}, {'paragraphId': 'p0030', 'paragraphText': 'Therein, the type of the vehicle may for example refer to a general type or purpose of the vehicle, such as the vehicle being a passenger car, a motorcycle, a truck, a van et cetera. The model of the vehicle may, for example, be more specifically identifying the vehicle. For example, the model of the vehicle may correspond to a specific model or model series of a specific manufacturer or brand of the vehicle.'}, {'paragraphId': 'p0031', 'paragraphText': 'According to several implementations, the at least one sensor dataset comprises the plurality of scan points. A subset of the plurality of scan points is determined, in particular by the at least one computing unit, such that each scan point of the subset is characterized by a distance from the ego-vehicle, which exceeds the first distance by at least a predefined minimum exceedance. The reference component is determined depending on the subset of the plurality of scan points.'}, {'paragraphId': 'p0032', 'paragraphText': 'In other words, the plurality of scan points may be filtered by their distance from the ego-vehicle and only those scan points, whose distances exceed the first distance by at least the minimum exceedance are taken into account for detecting and identifying the reference component, while other scan points of the plurality of scan points are not considered for this purpose.'}, {'paragraphId': 'p0033', 'paragraphText': 'In this way, it is ensured that there is no false detection of the reference component. For example, the rear wheels may also be detected by means of the at least one sensor dataset. By adjusting the minimum exceedance such that only desired reference components, such as front wheels or front mounted part of the underbody of the vehicle, are detected as the reference component. This ensures a high accuracy and a good estimation of the length of the vehicle.'}, {'paragraphId': 'p0034', 'paragraphText': 'The minimum exceedance may for example be greater than 1.5 m, for example it may lie between 1.5 m and 3 m.'}, {'paragraphId': 'p0035', 'paragraphText': 'According to several implementations, the subset of the plurality of scan points is determined such that each scan point of the subset is characterized by a height above a road surface, which lies within a predefined height interval.'}, {'paragraphId': 'p0036', 'paragraphText': 'In other words, the plurality of scan points may be filtered with respect to their height above the road surface. In this way, the position of the desired reference component may be more tightly defined, which leads to an increased reliability for detecting the reference component and therefore for an increased accuracy of the length estimation.'}, {'paragraphId': 'p0037', 'paragraphText': 'It is noted that the height filtering may be carried out in addition to or alternatively to the filtering with respect to the distance from the ego-vehicle.'}, {'paragraphId': 'p0038', 'paragraphText': 'The height interval is for example an interval [0, H], wherein H is equal to or smaller than 1 m, for example H is equal to or smaller than 0.8 m or equal to or smaller than 0.5 m.'}, {'paragraphId': 'p0039', 'paragraphText': 'According to several implementations each scan point of the plurality of scan points is characterized by a value concerning a target reflectivity.'}, {'paragraphId': 'p0040', 'paragraphText': 'The target reflectivity may be understood as a reflectivity or apparent reflectivity of a target, which reflects the corresponding light emitted by the lidar system, which is in turn detected by the lidar system to generate the corresponding scan point. For example, each such reflection may lead to a signal pulse of an optical detector, such as a photodiode or an avalanche photodiode, of the lidar system. Parameters describing the size or shape of the pulse, for example the echo-pulse width, EPW, may be interpreted as a measure for the target reflectivity and may be used as the value concerning the target reflectivity or to derive the value concerning the target reflectivity.'}, {'paragraphId': 'p0041', 'paragraphText': 'According to several implementations, the subset is determined such that each scan point of the subset is characterized by a value concerning the target reflectivity, which lies within a predefined reflectivity interval.'}, {'paragraphId': 'p0042', 'paragraphText': 'In other words, the scan points of the plurality of scan points are filtered with respect to their target reflectivity. This may be carried out in addition to the height filtering and/or the distance filtering or alternatively.'}, {'paragraphId': 'p0043', 'paragraphText': 'By means of the reflectivity filtering, the properties of the desired reference component may be defined in more detail. This may be particularly useful to distinguish scan points corresponding to the reference component from scan points on the road surface et cetera. For example, scan points, which are caused by reflections from the rim or tire of a wheel or a metallic component of the underbody may be in a specific typical range. This knowledge may be exploited to define the reflectivity interval.'}, {'paragraphId': 'p0044', 'paragraphText': 'As mentioned above, in the corresponding implementations, the reference component is determined depending on the subset of the plurality of scan points. This may be understood such that the one or more point clusters corresponding to the reference component are selected from the subset only but not from other points of the plurality of scan points.'}, {'paragraphId': 'p0045', 'paragraphText': 'According to several implementations, the third distance between the ego-vehicle and a side mirror component of the vehicle is determined depending on the at least one sensor dataset, in particular by the at least one computing unit. The length of the vehicle is estimated depending on the first distance, the second distance and the third distance.'}, {'paragraphId': 'p0046', 'paragraphText': 'The third distance may for example be considered as redundant information with respect to the second distance. Alternatively or in addition, the third distance may be used to determine the offset value or the model or type of the vehicle more accurately.'}, {'paragraphId': 'p0047', 'paragraphText': 'In particular, according to several implementations, the offset value is determined, for example selected from the database, depending on the third distance, for example depending on a further difference between the third distance and the first distance.'}, {'paragraphId': 'p0048', 'paragraphText': 'The third distance may, in general, be determined analogously as described with respect to the detection of the second. However, the corresponding filtering, in particular with respect to height, distance and reflectivity, may be adjusted accordingly.'}, {'paragraphId': 'p0049', 'paragraphText': 'According to a further aspect of the invention, a method for guiding an ego-vehicle at least in part automatically is provided. Therein, the ego-vehicle is guided at least in part automatically depending on a length of a vehicle driving in front of the ego-vehicle, wherein the length is estimated by using a method according to the invention for estimating a length of a vehicle.'}, {'paragraphId': 'p0050', 'paragraphText': 'In particular, at least one control signal for guiding the ego-vehicle at least in part automatically may be generated by at least one control unit of the ego-vehicle, for example an electronic vehicle guidance system of the ego-vehicle, depending on the length. The at least one control signal may be provided to one or more actuators of the ego-vehicle, which are configured to affect a longitudinal and/or lateral control of the ego-vehicle depending on the at least one control signal.'}, {'paragraphId': 'p0051', 'paragraphText': 'According to at least one implementation of the method for guiding an ego-vehicle at least in part automatically, at least one parameter for an overtaking maneuver is determined, in particular by the at least one computing unit, depending on the length and the overtaking maneuver is initiated, in particular by the electronic vehicle guidance system, according to the at least one parameter.'}, {'paragraphId': 'p0052', 'paragraphText': 'The at least one parameter may for example comprise a target speed or target acceleration of the ego-vehicle. For example, the longer the vehicle, the higher may be the target acceleration of the ego-vehicle to ensure a safe maneuver.'}, {'paragraphId': 'p0053', 'paragraphText': 'After the overtaking maneuver has been initiated, the at least one parameter may also be adapted according to additional sensor information obtained, for example from the lidar system. For example, also the length may be updated after the overtaking maneuver has been initiated and the at least one parameter may be adapted accordingly.'}, {'paragraphId': 'p0054', 'paragraphText': 'According to several implementations, an environment of the ego-vehicle comprising the vehicle is monitored by means of the lidar system and/or by means of a further environmental system of the ego-vehicle. After the overtaking maneuver is initiated and before the overtaking maneuver is completed, a lane change maneuver of the vehicle is recognized in particular by the at least one computing unit, depending on a result of the monitoring. A deceleration maneuver of the ego-vehicle is initiated depending on the length of the vehicle.'}, {'paragraphId': 'p0055', 'paragraphText': 'For example, at least one further parameter for the deceleration maneuver, for example a target deceleration, may be determined depending on the length. For example, the longer the vehicle, the higher is the target deceleration to avoid a collision when the vehicle is actually changing lanes while the ego-vehicle is carrying out the overtaking maneuver. According to a further aspect of the invention, a vehicle length estimation system for an ego-vehicle is provided. The vehicle length estimation system comprises a lidar system for the ego-vehicle, which is configured to generate at least one sensor dataset. The vehicle length estimation system comprises at least one computing unit, which is configured to determine a first distance between the ego-vehicle and a rear end of a vehicle in a field of view of the lidar system, which is driving in front of the ego-vehicle, depending on the at least one sensor dataset. The at least one computing unit is configured to determine a second distance between the ego-vehicle and a reference component, which is at least a part of a front wheel of the vehicle or an outer part of an underbody of the vehicle, depending on the at least one sensor dataset. The at least one computing unit is configured to estimate a length of the vehicle depending on the first distance and the second distance.'}, {'paragraphId': 'p0056', 'paragraphText': 'Further implementations of the vehicle length estimation system according to the invention follow directly from the various implementations of the method for estimating a length of a vehicle according to the invention and the various implementations of the method for guiding the ego-vehicle at least in part automatically according to the invention and vice versa, respectively. In particular, the vehicle length estimation system may be configured to carry out a method for estimating a length according to the invention or carries out such a method.'}, {'paragraphId': 'p0057', 'paragraphText': 'According to a further aspect of the invention, an electronic vehicle guidance system for an ego-vehicle is provided. The electronic vehicle guidance system comprises a vehicle length estimation system according to the invention. The electronic vehicle guidance system, for example the at least one computing unit, comprises at least one control unit, which is configured to generate at least one control signal for guiding the ego-vehicle at least in part automatically depending on the length of the vehicle.'}, {'paragraphId': 'p0058', 'paragraphText': 'According to a further aspect of the invention, a motor vehicle comprising a vehicle length estimation system according to the invention or an electronic vehicle guidance system according to the invention is provided. In this case, the motor vehicle corresponds to the ego-vehicle.'}, {'paragraphId': 'p0059', 'paragraphText': 'According to several implementations of the motor vehicle, the lidar system is mounted at a front end of the motor vehicle and a mounting height of the lidar system is equal to or less than 1 m. In particular, the mounting height is in the interval [0m, 1m].'}, {'paragraphId': 'p0060', 'paragraphText': 'A computing unit may in particular be understood as a data processing device, which comprises processing circuitry. The computing unit can therefore in particular process data to perform computing operations. This may also include operations to perform indexed accesses to a data structure, for example a look-up table, LUT.'}, {'paragraphId': 'p0061', 'paragraphText': 'In particular, the computing unit may include one or more computers, one or more microcontrollers, and/or one or more integrated circuits, for example, one or more application-specific integrated circuits, ASIC, one or more field-programmable gate arrays, FPGA, and/or one or more systems on a chip, SoC. The computing unit may also include one or more processors, for example one or more microprocessors, one or more central processing units, CPU, one or more graphics processing units, GPU, and/or one or more signal processors, in particular one or more digital signal processors, DSP. The computing unit may also include a physical or a virtual cluster of computers or other of said units.'}, {'paragraphId': 'p0062', 'paragraphText': 'In various embodiments, the computing unit includes one or more hardware and/or software interfaces and/or one or more memory units.'}, {'paragraphId': 'p0063', 'paragraphText': 'A memory unit may be implemented as a volatile data memory, for example a dynamic random access memory, DRAM, or a static random access memory, SRAM, or as a nonvolatile data memory, for example a read-only memory, ROM, a programmable read-only memory, PROM, an erasable read-only memory, EPROM, an electrically erasable read-only memory, EEPROM, a flash memory or flash EEPROM, a ferroelectric random access memory, FRAM, a magnetoresistive random access memory, MRAM, or a phase-change random access memory, PCRAM.'}, {'paragraphId': 'p0064', 'paragraphText': 'An electronic vehicle guidance system may be understood as an electronic system, configured to guide a vehicle in a fully automated or a fully autonomous manner and, in particular, without a manual intervention or control by a driver or user of the vehicle being necessary. The vehicle carries out all required functions, such as steering maneuvers, deceleration maneuvers and/or acceleration maneuvers as well as monitoring and recording the road traffic and corresponding reactions automatically. In particular, the electronic vehicle guidance system may implement a fully automatic or fully autonomous driving mode according to level 5 of the SAE J3016 classification. An electronic vehicle guidance system may also be implemented as an advanced driver assistance system, ADAS, assisting a driver for partially automatic or partially autonomous driving. In particular, the electronic vehicle guidance system may implement a partly automatic or partly autonomous driving mode according to levels 1 to 4 of the SAE J3016 classification. Here and in the following, SAE J3016 refers to the respective standard dated June 2018.'}, {'paragraphId': 'p0065', 'paragraphText': 'Guiding the vehicle at least in part automatically may therefore comprise guiding the vehicle according to a fully automatic or fully autonomous driving mode according to level 5 of the SAE J3016 classification. Guiding the vehicle at least in part automatically may also comprise guiding the vehicle according to a partly automatic or partly autonomous driving mode according to levels 1 to 4 of the SAE J3016 classification.'}, {'paragraphId': 'p0066', 'paragraphText': 'If it is mentioned in the present disclosure that a component of the vehicle length estimation system, the electronic vehicle guidance system or the motor vehicle according to the invention or the ego-vehicle, in particular the at least one computing unit, is adapted, configured or designed to, et cetera, to perform or realize a certain function, to achieve a certain effect or to serve a certain purpose, this can be understood such that the component, beyond being usable or suitable for this function, effect or purpose in principle or theoretically, is concretely and actually capable of executing or realizing the function, achieving the effect or serving the purpose by a corresponding adaptation, programming, physical design and so on.'}, {'paragraphId': 'p0067', 'paragraphText': 'Further features of the invention are apparent from the claims, the figures and the figure description. The features and combinations of features mentioned above in the description as well as the features and combinations of features mentioned below in the description of figures and/or shown in the figures may be comprised by the invention not only in the respective combination stated, but also in other combinations. In particular, embodiments and combinations of features, which do not have all the features of an originally formulated claim, may also be comprised by the invention. Moreover, embodiments and combinations of features which go beyond or deviate from the combinations of features set forth in the recitations of the claims may be comprised by the invention.'}, {'paragraphId': 'p0068', 'paragraphText': 'In the following, the invention will be explained in detail with reference to specific exemplary implementations and respective schematic drawings. In the drawings, identical or functionally identical elements may be denoted by the same reference signs. The description of identical or functionally identical elements is not necessarily repeated with respect to different figures. Therein, Fig. 1 shows schematically a side view of a vehicle, whose length may be estimated by an exemplary implementation of a method for estimating a length of a vehicle according to the invention; Fig.2 shows schematically an ego-vehicle and a vehicle driving in front of the ego-vehicle, wherein the ego-vehicle comprises an exemplary implementation of a vehicle length estimation system according to the invention; Fig. 3 shows schematically an image of the vehicle from a perspective of the ego-vehicle; Fig. 4 shows schematically a first view on a point cloud generated by a lidar system according to an exemplary implementation of a vehicle length estimation system according to the invention; Fig. 5 schematically a second view on a point cloud generated by a lidar system according to an exemplary implementation of a vehicle length estimation system according to the invention; Fig. 6 shows schematically a further image of the vehicle from a perspective of the ego-vehicle; and Fig. 7 shows schematically a view on a further point cloud generated by a lidar system according to an exemplary implementation of a vehicle length estimation system according to the invention.'}, {'paragraphId': 'p0069', 'paragraphText': 'In Fig. 2, a side view of an ego-vehicle 8 comprising an exemplary implementation of a vehicle length estimation system 13 according to the invention as well as a vehicle 1 driving in front of the ego-vehicle 8 on the same lane of a road with a road surface 7 as the ego-vehicle 8 are shown schematically.'}, {'paragraphId': 'p0070', 'paragraphText': 'The vehicle length estimation system 13 comprises a lidar system 11, which is mounted at a front end of the ego-vehicle 8, for example at or integrated in the bumper or hood of the ego-vehicle 8. The vehicle length estimation system 13 further comprises a computing unit 12, which is connected to the lidar system 11 and may receive at least one sensor dataset, in particular a point cloud, from the lidar system 11. The ego-vehicle 8 is located in a field of view 10 of the lidar system 11. Therefore, the point cloud depicts the vehicle 1.'}, {'paragraphId': 'p0071', 'paragraphText': 'Fig. 2 shows also schematically a sensor coordinate system with an x-axis, a y-axis and a z-axis. The x-axis, which may also be denoted as longitudinal axis, may for example be orientated in forward driving direction of the ego-vehicle 8. The z-axis may be perpendicular to the road surface 7 and perpendicular to the x-axis. The y-axis, which may also be denoted as lateral axis, is orientated perpendicular to the x-axis and the z-axis.'}, {'paragraphId': 'p0072', 'paragraphText': 'The point cloud comprises a plurality of scan points, which are characterized by respective three-dimensional coordinates. The three-dimensional coordinates may be given in the sensor coordinate system x, y, z or may be transformed accordingly. Therein, the origin of the sensor coordinate system x, y, z may be defined such that x = 0 corresponds to a position, which has zero distance in x-direction from the lidar system 11. In other words, the x-coordinates of objects in the environment of the ego-vehicle 8 correspond to the distance from the ego-vehicle 8.'}, {'paragraphId': 'p0073', 'paragraphText': 'Fig. 1 shows the vehicle 1 in more detail. A rear end of the vehicle 1, for example a rear bumper 2, is located at a distance X0 from the ego-vehicle 8. Rear wheels of the vehicle 1 may for example be located at a distance X1 > X0, side mirrors 4 of the vehicle 1 may be located at a distance X2 > X1, front wheels 5 of the vehicle 1 may be located at a distance X3 > X2 and a front end, for example a front end of a front bumper 3, of the vehicle 1 may be located at a distance X4>X3 with respect to the ego-vehicle 8. The actual length of the vehicle 1 is therefore by L = X4 - X0.'}, {'paragraphId': 'p0074', 'paragraphText': 'The computing unit 12 may apply one or more filtering algorithms and/or object detection algorithms to the point cloud to determine scan points or point clusters of scan points, which correspond to the front wheels 5 of the vehicle 1 or to another reference component, which is an outer part of an underbody 6 of the vehicle 1. The computing unit 12 may also determine a point cluster of scan points corresponding to the rear end of the vehicle 1. Consequently, the computing unit 12 may compute the distance X0 and the distance X3.'}, {'paragraphId': 'p0075', 'paragraphText': \"The computing unit 12 may then compute an estimated length L' of the vehicle 1 depending on the distance X0 and the distance X3. For example, the computing unit 12 may compute the difference X3 - X0 and add an offset value Xoff to obtain L' = X3 - X0 + Xoff to approximate L.\"}, {'paragraphId': 'p0076', 'paragraphText': 'Fig. 3 shows schematically an image of the vehicle 1 from a perspective of the ego-vehicle 8 and Fig. 4 shows a corresponding point cloud from a top view. The point cloud comprises a point cluster 9a, which corresponds to the rear end of the vehicle and two point clusters 9c, 9d corresponding to the two front wheels 5 of the vehicle 1. Furthermore, also a point cluster 9b corresponding to the rear wheels of the vehicle 1 is visible in Fig. 4. Fig. 5 shows a corresponding perspective side view of the point cloud of Fig. 4 such that the height information is also visible.'}, {'paragraphId': 'p0077', 'paragraphText': \"In some implementations, the computing unit 12 may also determine the distance X2 of the side mirrors 4 based on the point cloud. The computing unit 12 may for example compute the difference X2 - X0 and add a further offset value to obtain a further estimation to the length. The further estimation may for example be used for validation purposes or may be averaged with the estimated length L' to obtain a more reliable result. Alternatively or in addition, the difference X2 - X0 may be used by the computing unit 12 to select the offset value Xoff.\"}, {'paragraphId': 'p0078', 'paragraphText': 'Fig. 6 shows a further schematic image of the vehicle 1 from a perspective of the ego-vehicle 8. Fig. 7 shows a top view of the corresponding point cloud. Therein, the point cluster 9a corresponding to the rear end of the vehicle 1 is visible as well as two point clusters 9e, 9f corresponding to the two side mirrors 4.'}, {'paragraphId': 'p0079', 'paragraphText': 'As described, the invention allows to estimate the length of a vehicle, which drives in front of the ego-vehicle more reliably and/or accurately. A certain part of the length, namely the distance between the reference component and the rear end of the vehicle, is estimated very accurately based on lidar data. Also the side view mirrors of the vehicle can accurately be detected, which may also be used to estimate the width of the vehicle.'}, {'paragraphId': 'p0080', 'paragraphText': 'The invention is particularly beneficial for vehicles, which have a relatively large distance from the road surface, such as vans or small trucks. On the one hand, for such type of vehicles, the length is particularly important for example for overtaking maneuvers. The lidar system may be strategically mounted at the front of the ego-vehicle so that it can see under the front driving vehicle and reflectance from the front wheels may be used to calculate the estimated length.'}]}], 'claims': [{'claimId': 'c-en-0001', 'claimText': 'Method for estimating a length of a vehicle (1) driving in front of an ego-vehicle (8), wherein - at least one sensor dataset is generated by means of a lidar system (11) of the ego-vehicle (8), wherein the vehicle (1) is within a field of view (10) of the lidar system (11); - a first distance (X0) between the ego-vehicle (8) a rear end (2) of the vehicle (1) is determined depending on the at least one sensor dataset; characterized in that - a second distance (X3) between the ego-vehicle (8) and a reference component, which is at least a part of a front wheel (5) of the vehicle (1) or an outer part of an underbody (6) of the vehicle (1), is determined depending on the at least one sensor dataset; and - the length of the vehicle (1) is estimated depending on the first distance (X0) and the second distance (X3).'}, {'claimId': 'c-en-0002', 'claimText': 'Method according to claim 1, characterized in that the length of the vehicle (1) is estimated depending on a difference between the first distance (X0) and the second distance (X3).'}, {'claimId': 'c-en-0003', 'claimText': 'Method according to claim 2, characterized in that the length of the vehicle (1) is estimated as a sum of an offset value and the difference between the first distance (X0) and the second distance (X3).'}, {'claimId': 'c-en-0004', 'claimText': 'Method according to claim 3, characterized in that the offset value is selected from a database depending on the difference between the first distance (X0) and the second distance (X3).'}, {'claimId': 'c-en-0005', 'claimText': 'Method according to one of claims 3 or 4, characterized in that - a model or type of the vehicle (1) is determined depending on the at least one sensor dataset and/or depending on at least one camera image, which is generated by means of at least one camera of the ego-vehicle (8) and which depicts the vehicle (1); and - the offset value is determined depending on the model or type of the vehicle (1).'}, {'claimId': 'c-en-0006', 'claimText': 'Method according to one of the preceding claims, characterized in that - the at least one sensor dataset comprises a plurality of scan points (9a, 9b, 9c, 9d, 9e, 9f); - a subset of the plurality of scan points (9a, 9b, 9c, 9d, 9e, 9f) is determined, such that each scan point of the subset is characterized by a distance from the ego-vehicle (8), which exceeds the first distance by at least a predefined minimum exceedance; and - the reference component is determined depending on the subset of the plurality of scan points (9a, 9b, 9c, 9d, 9e, 9f).'}, {'claimId': 'c-en-0007', 'claimText': 'Method according to claim 6, characterized in that - the subset is determined such that each scan point of the subset is characterized by a height above a road surface (7), which lies within a predefined height interval; and/or - each scan point of the plurality of scan points (9a, 9b, 9c, 9d, 9e, 9f) is characterized by a value concerning a target reflectivity and the subset is determined such that each scan point of the subset is characterized by a value concerning the target reflectivity, which lies within a predefined reflectivity interval.'}, {'claimId': 'c-en-0008', 'claimText': 'Method according to one of the preceding claims, characterized in that - a third distance (X2) between the ego-vehicle (8) and a side mirror component (4) of the vehicle (1) is determined depending on the at least one sensor dataset; and - the length of the vehicle (1) is estimated depending on the first distance (X0), the second distance (X3) and the third distance (X2).'}, {'claimId': 'c-en-0009', 'claimText': 'Method for guiding an ego-vehicle (8) at least in part automatically depending on a length of a vehicle (1) driving in front of the ego-vehicle (8), characterized in that the length is estimated by using a method according to one of the preceding claims.'}, {'claimId': 'c-en-0010', 'claimText': 'Method according to claim 9, characterized in that at least one parameter for an overtaking maneuver is determined depending on the length and the overtaking maneuver is initiated according to the at least one parameter.'}, {'claimId': 'c-en-0011', 'claimText': 'Method according to claim 10, characterized in that - an environment of the ego-vehicle (8) is monitored by means of the lidar system (11) and/or an environmental sensor system of the ego-vehicle (8); - after the overtaking maneuver is initiated and before the overtaking maneuver is completed, a lane change maneuver of the vehicle (1) is recognized depending on a result of the monitoring; - a deceleration maneuver of the ego-vehicle (8) is initiated depending on the length of the vehicle (1).'}, {'claimId': 'c-en-0012', 'claimText': 'Vehicle length estimation system (13) for an ego-vehicle (8), comprising - a lidar system (11) for the ego-vehicle (8), which is configured to generate at least one sensor dataset; - at least one computing unit (12), which is configured to determine a first distance (X0) between the ego-vehicle (8) a rear end (2) of a vehicle (1) in a field of view (10) of the lidar system (11), which is driving in front of the ego-vehicle (8), depending on the at least one sensor dataset; characterized in that the at least one computing unit (12) is configured to - determine a second distance (X3) between the ego-vehicle (8) and a reference component, which is at least a part of a front wheel (5) of the vehicle (1) or an outer part of an underbody (6) of the vehicle (1), depending on the at least one sensor dataset; and - to estimate a length of the vehicle (1) depending on the first distance (X0) and the second distance (X3).'}, {'claimId': 'c-en-0013', 'claimText': 'Electronic vehicle guidance system for an ego-vehicle (8), which comprises a vehicle length estimation system (13) according to claim 12 and at least one control unit, which is configured to generate at least one control signal for guiding the ego-vehicle (8) at least in part automatically depending on the length of the vehicle (1).'}, {'claimId': 'c-en-0014', 'claimText': 'Motor vehicle comprising a vehicle length estimation system (13) according to claim 12 or an electronic vehicle guidance system according to claim 13.'}, {'claimId': 'c-en-0015', 'claimText': 'Motor vehicle according to claim 14, characterized in that the lidar system (11) is mounted at a front end of the motor vehicle and a mounting height of the lidar system (11) is less than 1 m.'}], 'resourceId': 'EP_EP23156734A1'}}\n"
          ]
        }
      ]
    }
  ]
}